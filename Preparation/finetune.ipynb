{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26da2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac4bda",
   "metadata": {},
   "source": [
    "### General Context\n",
    "\n",
    "- DocRED: 1000 for the train set, 100 for the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80c80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the name mentions in each document\n",
    "\n",
    "total = defaultdict(list)\n",
    "for split in ['train', 'dev']:\n",
    "    for doc in json.load(open(f'../Data/Finetune/Raw/General/{split}.json', 'r')):\n",
    "        names = defaultdict(list)\n",
    "        for entity in doc['vertexSet']:\n",
    "            for mention in entity:\n",
    "                if mention['type'] == 'PER': names[mention['sent_id']].append(tuple(mention['pos']))\n",
    "        if len(names) == 0: continue\n",
    "        \n",
    "        names = {k:[v for v in sorted(vs, key=lambda v:v[0], reverse=True)] for k,vs in names.items()}\n",
    "        nameID = 1\n",
    "        for sid, spans in names.items():\n",
    "            for span in spans:\n",
    "                doc['sents'][sid][span[0]] = f'**NAME-{nameID}A**'\n",
    "                nameID += 1\n",
    "            for span in spans:\n",
    "                del doc['sents'][sid][span[0]+1:span[1]]\n",
    "        total[split].append(' '.join([' '.join(sent) for sent in doc['sents']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c8ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split the dataset into a train set (1000 docs) and a dev set (100 docs)\n",
    "\n",
    "with jsonlines.open('../Data/Finetune/Input/context-general.jsonl', 'w') as writer:\n",
    "    for split, size in zip(['train', 'dev'], [1000, 100]):\n",
    "        np.random.shuffle(total[split])\n",
    "        writer.write_all([{'split':split, 'template':template} for template in total[split][:size]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab501a9b",
   "metadata": {},
   "source": [
    "### Clinical Context\n",
    "\n",
    "- 2014 i2b2 de-identification challenge: 1000 for the train set, 100 for the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696aee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the name mentions in each document\n",
    "\n",
    "total = []\n",
    "for filename in os.listdir('../Data/Finetune/Raw/Clinical'):\n",
    "    doc = BeautifulSoup(open(f'../Data/Finetune/Raw/Clinical/{filename}', 'r').read(), 'xml')\n",
    "    names = [(int(name.get('start')), int(name.get('end'))) for name in doc.find('TAGS').find_all('NAME')]\n",
    "    if names == 0: continue\n",
    "    \n",
    "    text = list(doc.find('TEXT').text)\n",
    "    for nameID, (start, end) in enumerate(sorted(names, key=lambda x:x[0], reverse=True)):\n",
    "        text[start:end] = f'**NAME-{nameID+1}A**'\n",
    "    total.append(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83af6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split the dataset into a train set (1000 docs) and a dev set (100 docs)\n",
    "\n",
    "np.random.shuffle(total)\n",
    "train, dev = total[:1000], total[1000:1100]\n",
    "with jsonlines.open('../Data/Finetune/Input/context-clinical.jsonl', 'w') as writer:\n",
    "    writer.write_all([{'split':'train', 'template':template} for template in train])\n",
    "    writer.write_all([{'split':'dev', 'template':template} for template in dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211613e",
   "metadata": {},
   "source": [
    "### Diverse Names\n",
    "\n",
    "- 10 first/last names from each of the 16 sets in names-first/last.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c6145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 16 name sets (first and last names)\n",
    "\n",
    "names = defaultdict(lambda: defaultdict(list))\n",
    "for part in ['first', 'last']:\n",
    "    with open(f'../Data/General/Input/names-{part}.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        _ = next(reader)\n",
    "        for row in reader:\n",
    "            setID, name = int(row[0]), row[-3]\n",
    "            names[part][setID].append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34637f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each set into half\n",
    "\n",
    "file_train, file_test = open('../Data/Finetune/Input/names-diverse.csv', 'w'), open('../Data/Finetune/Input/names-test.csv', 'w')\n",
    "writer_train, writer_test = csv.writer(file_train), csv.writer(file_test)\n",
    "for part in ['first', 'last']:\n",
    "    for setID, set_ in names[part].items():\n",
    "        np.random.shuffle(set_)\n",
    "        \n",
    "        for name in set_[:10]:\n",
    "            writer_train.writerow([part, setID, name])\n",
    "        for name in set_[10:]:\n",
    "            writer_test.writerow([part, setID, name])    \n",
    "file_train.close(); file_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c11f6",
   "metadata": {},
   "source": [
    "### Popular Names\n",
    "\n",
    "- 160 most popular first names from the 1940s, 1970s, 2000s that do not appear in names-first.csv\n",
    "- 160 most popular surnames from the 2000s that do not appear in names-last.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54c7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of first and last names\n",
    "\n",
    "years, duration = [1940, 1970, 2000], 10\n",
    "first_count = defaultdict(int)\n",
    "for year in years:\n",
    "    for i in range(duration):\n",
    "        with open(f'../Data/General/Raw/Name/firstnames-socialsecurity-{year+i}.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                first, _, count = line[:-1].split(',')\n",
    "                first_count[first] += int(count)\n",
    "first_count = [(first, count) for first, count in sorted(first_count.items(), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "last_count = []\n",
    "with open('../Data/General/Raw/Name/surnames-census-2000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    _ = next(reader)\n",
    "    for row in reader:\n",
    "        last, count = row[0], int(row[2])\n",
    "        last = last[0] + last[1:].lower()\n",
    "        last_count.append((last, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bb47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the 160 most popular first/last names that do not appear in names-first/last.csv\n",
    "\n",
    "popular = defaultdict(list)\n",
    "diverse = {part: {each for _, set_ in names[part].items() for each in set_} for part in names}\n",
    "for name_count, part in zip([first_count, last_count], ['first', 'last']):\n",
    "    for name, count in name_count:\n",
    "        if name not in diverse[part]:\n",
    "            popular[part].append((name, count))\n",
    "            if len(popular[part]) >= 160: break\n",
    "            \n",
    "with open('../Data/Finetune/Input/names-popular.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for part in popular:\n",
    "        for name, count in popular[part]:\n",
    "            writer.writerow([part, count, name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeea4f2",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "- inputs-{type_context}+{type_name}.csv: {(train/dev, index): note}; \n",
    "- labels-{type_context}+{type_name}.csv: {(train/dev, index): {(start, end): (name, tag)}}\n",
    "- 4 Settings: \n",
    "    - General Context + Popular Names\n",
    "    - General Context + Diverse Names\n",
    "    - Clinical Context + Popular Names\n",
    "    - Clinical Context + Diverse Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0cafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namer:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.name_tag = ('**NAME-', 11)\n",
    "                    \n",
    "    def find_occurrences(self, note, tag):\n",
    "        \n",
    "        pattern, length = tag\n",
    "        occurrences = []\n",
    "        index = note.find(pattern)\n",
    "        while index != -1:\n",
    "            occurrences.append((index, index+length, note[index:index+length]))\n",
    "            index = note.find(pattern, index+length)\n",
    "        return occurrences\n",
    "\n",
    "    def sample_name(self, names):\n",
    "        \n",
    "        for each in names.values():\n",
    "            np.random.shuffle(each)\n",
    "        return names\n",
    "                        \n",
    "    # identify the name tags, sample the names, replace the name tags, return the notes and names\n",
    "    def __call__(self, names, templates):\n",
    "        \n",
    "        input_notes, input_labels = {}, defaultdict(dict)\n",
    "        for templateID, template in templates.items():\n",
    "            tags = {tag for _, _, tag in self.find_occurrences(template, self.name_tag)}\n",
    "            names = self.sample_name(names)\n",
    "                    \n",
    "            name2tag = {}\n",
    "            input_notes[templateID] = template\n",
    "            for tag in tags:\n",
    "                personID = int(tag[7])\n",
    "                name = ' '.join([names['first'][personID], names['last'][personID]])\n",
    "                name2tag[name] = tag\n",
    "                input_notes[templateID] = input_notes[templateID].replace(tag, name)\n",
    "\n",
    "            for name in name2tag:\n",
    "                for start, end, _ in self.find_occurrences(input_notes[templateID], (name, len(name))):\n",
    "                    input_labels[templateID][(start, end)] = (name, name2tag[name])\n",
    "                            \n",
    "            name_overlap = set()\n",
    "            positions = list(input_labels[templateID].keys())\n",
    "            for i in range(len(positions)):\n",
    "                for j in range(i+1, len(positions)):\n",
    "                    if positions[i][0]<=positions[j][0] and positions[i][1]>=positions[j][1]: name_overlap.add(positions[j])\n",
    "            for start, end in name_overlap:\n",
    "                del input_labels[templateID][(start, end)]\n",
    "                        \n",
    "        return input_notes, input_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a1b3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the templates and names, populate the templates with names, save the inputs and labels\n",
    "\n",
    "for type_context in ['general', 'clinical']:\n",
    "    for type_name in ['popular', 'diverse']:\n",
    "\n",
    "        templates = {}\n",
    "        with jsonlines.open(f'../Data/Finetune/Input/context-{type_context}.jsonl', 'r') as reader:\n",
    "            for templateID, line in enumerate(reader):\n",
    "                templates[(line['split'], templateID if line['split']=='train' else templateID-1000)] = line['template']\n",
    "\n",
    "        names = defaultdict(list)\n",
    "        with open(f'../Data/Finetune/Input/names-{type_name}.csv', 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                part, _, name = row\n",
    "                names[part].append(name)\n",
    "\n",
    "        namer = Namer()\n",
    "        inputs, labels = namer(names, templates)\n",
    "\n",
    "        with jsonlines.open(f'../Data/Finetune/Input/inputs-{type_context}+{type_name}.jsonl', 'w') as writer:\n",
    "            writer.write_all([{'ID':ID, 'note':note} for ID, note in inputs.items()])\n",
    "        with jsonlines.open(f'../Data/Finetune/Input/labels-{type_context}+{type_name}.jsonl', 'w') as writer:\n",
    "            writer.write_all([{'ID':ID, 'position':position, 'name':name} for ID, mentions in labels.items() for position, name in mentions.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269de1c",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "- 100 templates * 1 copy * 16 sets = 1600 notes for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0249437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.copy_size = 1\n",
    "        self.name_tag = ('**NAME-', 11)\n",
    "        self.other_tags = {'**AGE**':'38', '**CONTACT**':'0123456789', '**DATE**':'2100/01/01', \n",
    "                           '**HOSPITAL**':'General Hospital', '**ID**':'100', '**LANGUAGE**':'English', \n",
    "                           '**LOCATION**':'Building 1', '**OTHER**':'_', '**PROFESSION**':'worker'}\n",
    "        \n",
    "        self.first2last = {1:1, 2:1, 3:2, 4:2, 5:3, 6:3, 7:4, 8:4, 9:5, 10:5, 11:6, 12:6, 13:1, 14:1, 15:1, 16:1}\n",
    "        self.first_sets, self.last_sets = defaultdict(list), defaultdict(list)\n",
    "        with open('../Data/Finetune/Input/names-test.csv', 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                part, setID, name = row\n",
    "                if part == 'first': self.first_sets[int(setID)].append(name)\n",
    "                elif part == 'last': self.last_sets[int(setID)].append(name)\n",
    "                    \n",
    "    def replace_irrelevant(self, template):\n",
    "        \n",
    "        for tag, fake in self.other_tags.items():\n",
    "            template = template.replace(tag, fake)\n",
    "        return template\n",
    "                    \n",
    "    def find_occurrences(self, note, tag):\n",
    "        \n",
    "        pattern, length = tag\n",
    "        occurrences = []\n",
    "        index = note.find(pattern)\n",
    "        while index != -1:\n",
    "            occurrences.append((index, index+length, note[index:index+length]))\n",
    "            index = note.find(pattern, index+length)\n",
    "        return occurrences\n",
    "                    \n",
    "    def sample_name(self, setID):\n",
    "        \n",
    "        names = {'F':self.first_sets[setID], 'L': self.last_sets[self.first2last[setID]]}         \n",
    "        for each in names.values():\n",
    "            np.random.shuffle(each)\n",
    "        return names\n",
    "                        \n",
    "    # replace the irrelevant tags, identify the name tags, sample the names, replace the name tags, return the notes and names\n",
    "    def __call__(self, templates):\n",
    "        \n",
    "        input_notes, input_labels = {}, defaultdict(dict)\n",
    "        for templateID, row in templates.iterrows():\n",
    "            template = self.replace_irrelevant(row['text'])\n",
    "            tags = {tag for _, _, tag in self.find_occurrences(template, self.name_tag)}\n",
    "            for setID in self.first2last:\n",
    "                for copyID in range(self.copy_size):\n",
    "                    names = self.sample_name(setID)\n",
    "                    \n",
    "                    name2tag, name2setID = {}, {}\n",
    "                    input_notes[(templateID, setID, copyID)] = template\n",
    "                    for tag in tags:\n",
    "                        personID = int(tag[7])\n",
    "                        name = ' '.join([names['F'][personID], names['L'][personID]])\n",
    "                        name2tag[name] = tag\n",
    "                        name2setID[name] = setID\n",
    "                        input_notes[(templateID, setID, copyID)] = input_notes[(templateID, setID, copyID)].replace(tag, name)\n",
    "                        \n",
    "                    for name in name2tag:\n",
    "                        for start, end, _ in self.find_occurrences(input_notes[(templateID, setID, copyID)], (name, len(name))):\n",
    "                            input_labels[(templateID, setID, copyID)][(start, end)] = (name, name2tag[name], name2setID[name])\n",
    "                            \n",
    "                    name_overlap = set()\n",
    "                    positions = list(input_labels[(templateID, setID, copyID)].keys())\n",
    "                    for i in range(len(positions)):\n",
    "                        for j in range(i+1, len(positions)):\n",
    "                            if positions[i][0]<=positions[j][0] and positions[i][1]>=positions[j][1]: name_overlap.add(positions[j])\n",
    "                    for start, end in name_overlap:\n",
    "                        del input_labels[(templateID, setID, copyID)][(start, end)]\n",
    "                        \n",
    "        return input_notes, input_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a65827c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and save the test inputs and labels\n",
    "\n",
    "namer = Namer()\n",
    "templates = pd.read_csv('../Data/General/Input/notes-base.csv')\n",
    "test_inputs, test_labels = namer(templates)\n",
    "\n",
    "with jsonlines.open(f'../Data/Finetune/Input/inputs-test.jsonl', 'w') as writer:\n",
    "    writer.write_all([{'ID':ID, 'note':note} for ID, note in test_inputs.items()])\n",
    "with jsonlines.open(f'../Data/Finetune/Input/labels-test.jsonl', 'w') as writer:\n",
    "    writer.write_all([{'ID':ID, 'position':position, 'name':name} for ID, mentions in test_labels.items() for position, name in mentions.items()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
