{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611a7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import jsonlines\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from IPython.utils import io\n",
    "from neuroner import neuromodel\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "tf.compat.v1.random.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb7513",
   "metadata": {},
   "source": [
    "### General Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec31434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the inputs\n",
    "\n",
    "notes = {}\n",
    "with jsonlines.open('../Data/General/Input/notes-input.jsonl', 'r') as reader:\n",
    "    for line in reader:\n",
    "        notes[tuple(line['ID'])] = line['note']\n",
    "        with open('External/NeuroNER/Data/General/deploy/' + '_'.join(map(str, line['ID'])) + '.txt', 'w') as file:\n",
    "            file.write(line['note'])\n",
    "        with open('External/NeuroNER/Data/General/deploy/' + '_'.join(map(str, line['ID'])) + '.ann', 'w') as file:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab544442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "# neuromodel.fetch_model('i2b2_2014_glove_spacy_bioes')\n",
    "nn = neuromodel.NeuroNER(\n",
    "    train_model=False, use_pretrained_model=True,\n",
    "    parameters_filepath='External/NeuroNER/Model/original+original/parameters.ini', \n",
    "    pretrained_model_folder='External/NeuroNER/Model/original+original/',\n",
    "    token_pretrained_embedding_filepath='External/NeuroNER/Data/Embedding/glove.6B.100d.txt',\n",
    "    dataset_text_folder='External/NeuroNER/Data/General',\n",
    "    output_folder='External/NeuroNER/Data/General')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41cf16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Processing Note 0\n",
      "Finish Processing Note 1000\n",
      "Finish Processing Note 2000\n",
      "Finish Processing Note 3000\n",
      "Finish Processing Note 4000\n",
      "Finish Processing Note 5000\n",
      "Finish Processing Note 6000\n",
      "Finish Processing Note 7000\n",
      "Finish Processing Note 8000\n",
      "Finish Processing Note 9000\n",
      "Finish Processing Note 10000\n",
      "Finish Processing Note 11000\n",
      "Finish Processing Note 12000\n",
      "Finish Processing Note 13000\n",
      "Finish Processing Note 14000\n",
      "Finish Processing Note 15000\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "outputs = defaultdict(dict)\n",
    "for count, (ID, note) in enumerate(notes.items()):\n",
    "    \n",
    "    with io.capture_output() as captured:\n",
    "        ents = nn.predict(note)\n",
    "    \n",
    "    for ent in ents:\n",
    "        if ent['type'] in {'DOCTOR', 'PATIENT'}:\n",
    "            outputs[ID][(ent['start'], ent['end'])] = ent['text']\n",
    "            \n",
    "    if count % 1000 == 0: print(f'Finish Processing Note {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81daed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outputs\n",
    "\n",
    "with jsonlines.open('../Data/General/Output/notes-NeuroNER.jsonl', 'w') as writer:\n",
    "    writer.write_all([{'ID':list(ID), 'position':list(position), 'name':[name]} for ID, preds in outputs.items() for position, name in preds.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b759585",
   "metadata": {},
   "source": [
    "### Polysemy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773d0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the inputs\n",
    "\n",
    "notes = {}\n",
    "with jsonlines.open('../Data/Polysemy/Input/polysemies-input.jsonl', 'r') as reader:\n",
    "    for line in reader:\n",
    "        notes[tuple(line['ID'])] = line['note']\n",
    "        with open('External/NeuroNER/Data/Polysemy/deploy/' + '_'.join(map(str, line['ID'])) + '.txt', 'w') as file:\n",
    "            file.write(line['note'])\n",
    "        with open('External/NeuroNER/Data/Polysemy/deploy/' + '_'.join(map(str, line['ID'])) + '.ann', 'w') as file:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c56603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "# neuromodel.fetch_model('i2b2_2014_glove_spacy_bioes')\n",
    "nn = neuromodel.NeuroNER(\n",
    "    train_model=False, use_pretrained_model=True,\n",
    "    parameters_filepath='External/NeuroNER/Model/original+original/parameters.ini', \n",
    "    pretrained_model_folder='External/NeuroNER/Model/original+original',\n",
    "    token_pretrained_embedding_filepath='External/NeuroNER/Data/Embedding/glove.6B.100d.txt',\n",
    "    dataset_text_folder='External/NeuroNER/Data/Polysemy',\n",
    "    output_folder='External/NeuroNER/Data/Polysemy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab41e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Processing Note 0\n",
      "Finish Processing Note 100\n",
      "Finish Processing Note 200\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "outputs = defaultdict(dict)\n",
    "for count, (ID, note) in enumerate(notes.items()):\n",
    "    \n",
    "    with io.capture_output() as captured:\n",
    "        ents = nn.predict(note)\n",
    "    \n",
    "    for ent in ents:\n",
    "        if ent['type'] in {'DOCTOR', 'PATIENT'}:\n",
    "            outputs[ID][(ent['start'], ent['end'])] = ent['text']\n",
    "            \n",
    "    if count % 100 == 0: print(f'Finish Processing Note {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1a6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outputs\n",
    "\n",
    "with jsonlines.open('../Data/Polysemy/Output/polysemies-NeuroNER.jsonl', 'w') as writer:\n",
    "    writer.write_all([{'ID':list(ID), 'position':list(position), 'name':[name]} for ID, preds in outputs.items() for position, name in preds.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfd568",
   "metadata": {},
   "source": [
    "### Finetuning Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf84d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the inputs\n",
    "\n",
    "def prepare(seed, type_context, type_name):\n",
    "    \n",
    "    for split in ['train', 'valid', 'deploy']:\n",
    "        Path(f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/{split}').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with jsonlines.open(f'../Data/Finetune/Input/inputs-{type_context}+{type_name}.jsonl', 'r') as reader:\n",
    "        for line in reader:\n",
    "            split, ID = line['ID']\n",
    "            split = split if split == 'train' else 'valid'\n",
    "            with codecs.open(f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/{split}/{ID}.txt', 'w', 'UTF-8') as file:\n",
    "                file.write(line['note'])\n",
    "\n",
    "    labels = defaultdict(dict)\n",
    "    with jsonlines.open(f'../Data/Finetune/Input/labels-{type_context}+{type_name}.jsonl', 'r') as reader:\n",
    "        for line in reader:\n",
    "            ID, position, name = map(lambda x:tuple(x), line.values())\n",
    "            labels[ID][position] = name\n",
    "    for ID, mentions in labels.items():\n",
    "        split, ID = ID\n",
    "        split = split if split == 'train' else 'valid'\n",
    "        with codecs.open(f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/{split}/{ID}.ann', 'w', 'utf-8') as file:\n",
    "            for mentionID, (position, name) in enumerate(mentions.items()):\n",
    "                label = random.choice(['PATIENT', 'DOCTOR'])\n",
    "                file.write(f'T{mentionID}\\t{label} {position[0]} {position[1]}\\t{name[0]}\\n')\n",
    "                \n",
    "    notes = {}\n",
    "    with jsonlines.open('../Data/Finetune/Input/inputs-test.jsonl', 'r') as reader:\n",
    "        for line in reader:\n",
    "            notes[tuple(line['ID'])] = line['note']\n",
    "            with open(f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/deploy/' + '_'.join(map(str, line['ID'])) + '.txt', 'w') as file:\n",
    "                file.write(line['note'])\n",
    "            with open(f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/deploy/' + '_'.join(map(str, line['ID'])) + '.ann', 'w') as file:\n",
    "                pass\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9efb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning\n",
    "\n",
    "def finetune(seed, type_context, type_name):\n",
    "    \n",
    "    # load the inputs\n",
    "    random.seed(seed)\n",
    "    tf.compat.v1.random.set_random_seed(seed)\n",
    "    notes = prepare(seed, type_context, type_name)\n",
    "    print('Finish Loading the Inputs')\n",
    "    \n",
    "    # load and finetune the model\n",
    "    nn = neuromodel.NeuroNER(\n",
    "        train_model=True, use_pretrained_model=True,\n",
    "        maximum_number_of_epochs=5, patience=2,\n",
    "        parameters_filepath='External/NeuroNER/Model/parameters.ini', \n",
    "        pretrained_model_folder='External/NeuroNER/Model/',\n",
    "        token_pretrained_embedding_filepath='External/NeuroNER/Data/Embedding/glove.6B.100d.txt',\n",
    "        dataset_text_folder=f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/',\n",
    "        output_folder=f'External/NeuroNER/Data/Finetune/{type_context}+{type_name}/{seed}/')\n",
    "    nn.fit()\n",
    "    print('Finish Finetuning the Model')\n",
    "    \n",
    "    # make predictions\n",
    "    outputs = defaultdict(dict)\n",
    "    for count, (ID, note) in enumerate(notes.items()):\n",
    "        with io.capture_output() as captured:\n",
    "            ents = nn.predict(note)\n",
    "        for ent in ents:\n",
    "            if ent['type'] in {'DOCTOR', 'PATIENT'}:\n",
    "                outputs[ID][(ent['start'], ent['end'])] = ent['text']\n",
    "        if count % 100 == 0: print(f'Finish Processing Note {count}')\n",
    "\n",
    "    # save the outputs\n",
    "    with jsonlines.open(f'../Data/Finetune/Output/finetunes-{type_context}+{type_name}-NeuroNER-{seed}.jsonl', 'w') as writer:\n",
    "        writer.write_all([{'ID':list(ID), 'position':list(position), 'name':[name]} for ID, preds in outputs.items() for position, name in preds.items()])\n",
    "    print('Finish Saving the Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6bf88cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start seed = 0 | Context = general | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 0 | Context = general | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 0 | Context = clinical | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 0 | Context = clinical | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 1 | Context = general | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 1 | Context = general | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 1 | Context = clinical | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 1 | Context = clinical | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 2 | Context = general | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 2 | Context = general | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 2 | Context = clinical | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 2 | Context = clinical | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 3 | Context = general | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 3 | Context = general | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 3 | Context = clinical | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 3 | Context = clinical | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 4 | Context = general | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 4 | Context = general | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 4 | Context = clinical | Name = popular\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n",
      "Start seed = 4 | Context = clinical | Name = diverse\n",
      "Finish Loading the Inputs\n",
      "Finish Finetuning the Model\n",
      "Finish Saving the Predictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    for type_context in ['general', 'clinical']:\n",
    "        for type_name in ['popular', 'diverse']:\n",
    "            print(f'Start seed = {seed} | Context = {type_context} | Name = {type_name}')\n",
    "            finetune(seed, type_context, type_name)\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
